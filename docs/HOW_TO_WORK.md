Analyzer Machine — HOW_TO_WORK (Cursor-first)

Цель документа
Этот документ описывает, как вести проект Analyzer Machine в Cursor так, чтобы:
— расчёты выполнялись программно (детерминированно), а не “на ощущениях” модели;
— LLM использовалась только для интерпретации уже посчитанных агрегатов и подготовки текста отчёта;
— стоимость по токенам была предсказуемой и минимальной;
— работа в Cursor была воспроизводимой: маленькие задачи, понятный DoD, проверяемость.

1) Ментальная модель Cursor: что он делает, а что нет
Cursor — это IDE (по сути VS Code) с AI-слоем. Важно:
— Cursor помогает читать код, предлагать изменения, строить план, искать по проекту, писать диффы.
— Cursor не является “автоматическим исполнителем” по умолчанию: команды в терминале обычно запускаешь ты, но прямо внутри Cursor, без копипаста “между окнами”.
— Поэтому “всё в одном месте” выглядит так: Chat/Agent предлагает команду → ты запускаешь её в встроенном Terminal → результат вставляешь в Debug/Agent, если нужно.

2) Карта интерфейса Cursor (минимум, что нужно)
В типичной раскладке:
— Editor (центр): файлы проекта.
— Sidebar (слева): файловое дерево + Agents.
— Terminal (снизу): выполнение команд, тестов, линтеров.
— Changes / Source Control (справа или отдельной вкладкой): staged/unstaged изменения, commit/push.
— Agent chat (панель слева/сверху): постановка задач и взаимодействие.

3) Режимы чата: Ask / Plan / Agent / Debug
Важно: названия и UI могут отличаться по версиям, но смысл устойчивый.

Ask
— Быстрые вопросы/разбор.
— Не требуй “сделай PR и запусти тесты”. Здесь лучше “объясни” / “предложи варианты”.

Plan
— Режим проектирования/планирования.
— Используй для: декомпозиции задачи, списка файлов, DoD, рисков, оценок по данным/токенам.
— Артефакт этого режима: план в docs/tasks/XXXX.md (см. шаблон ниже).

Agent
— Режим выполнения: внесение правок в файлы, создание новых файлов, подготовка диффов.
— Правило: Agent работает по плану (Plan), а не “как получится”.
— После правок: обязательно прогоняем команды проверки (ты в терминале), фиксируем результат в задаче.

Debug
— Режим “вот лог/traceback — почини”.
— Вход: точный traceback/лог + команда, которой ты это получил + версия Python/зависимостей.
— Выход: минимальный фикс + объяснение причины + проверка.

4) Как ставить задачи так, чтобы Cursor делал правильно
Главный принцип: задача = один результат, один вертикальный срез.

4.1. Формат задачи (Task Brief)
Копируй это в чат (Plan или Agent), подставляя значения.

TASK: <краткое название>
GOAL (что получить):
— …
CONTEXT (где в проекте):
— ветка/папки/команды, которые уже есть
INPUTS (какие данные/диапазоны):
— client=<…>, date1=<…>, date2=<…>
CONSTRAINTS (жёсткие правила):
— цифры считаем кодом, не моделью
— не печатать/не логировать токены и секреты
— запросы к API минимизировать, использовать data_cache
— лимит на LLM: максимум N вызовов, вход только агрегаты (JSON)
OUTPUT (что должно появиться):
— файл(ы) / команда CLI / формат json/csv/md
DOD (Definition of Done):
— команды проверки: <…>
— пример запуска: <…>
— ожидаемые файлы в data_cache/…: <…>
— тест(ы) или self-check: <…>

4.2. Обязательное правило “DoD прежде кода”
Перед тем как просить что-то реализовать:
— сформулируй DoD (как проверить, что работает).
— перечисли команды проверки (в нашем проекте это обычно python -m app.cli …).
— укажи формат артефактов в data_cache/ и reports/.

5) Как держать точность и экономить токены
5.1. Разделяй “данные” и “интерпретацию”
— Данные: получаем через API, нормализуем, сохраняем в data_cache/.
— Аналитика/метрики: считаем в Python (дельты, вклады, топы, корреляции).
— Текст: LLM получает только результат расчётов (короткий JSON с топ-N), и пишет вывод.

5.2. “Сырьё не кормим модели”
Никогда не передавай в LLM:
— сырые ответы API целиком;
— большие списки страниц/запросов > 200 строк;
— логи запросов с заголовками Authorization.

Вместо этого:
— агрегируй (top-N, percentiles, sum, delta, contribution).
— округляй метрики до разумной точности (например, 1 знак после запятой для процентов).
— ограничивай поля: только то, что нужно для вывода.

5.3. Кэш обязателен
Любой запрос к Метрике должен:
— сохранять raw и normalized в data_cache/<client>/…
— уметь читать из кэша (или иметь отдельный флаг --refresh).

5.4. LLM как “последний шаг”
Рекомендованный паттерн:
1) fetch → cache
2) normalize
3) compute (analysis)
4) render: таблицы/JSON/CSV
5) (опционально) LLM summary на основе compute-результатов

6) Практика ведения проекта (как это делают “профессионально”)
6.1. Мелкие итерации
— 1 задача = 1 команда CLI или 1 аналитический отчёт.
— Коммиты маленькие и тематические.

6.2. Branch per task (даже локально)
Даже если GitHub только backup:
— заведи ветку feature/<short-name> или fix/<short-name>
— закончил — слей в main

6.3. Всегда фиксируй “контракт” в docs/
Если добавляешь команду:
— обновляешь docs/spec.md (что делает команда)
— обновляешь docs/api_catalog.md (какие запросы, параметры, где кэш)
— обновляешь docs/analysis_rules.md (какие метрики считаем и как)

6.4. “Тесты” без полноценного тест-раннера — допустимо, но формализуй
Если пока нет pytest:
— делай self-check: команда запуска + ожидаемые файлы + проверка нескольких значений
— фиксируй это в docs/tasks/…

7) Где настраивается “нормальная модель” в Cursor
Есть два типовых подхода:
— использовать модели, доступные в подписке Cursor;
— подключить собственные ключи провайдеров (если твой тариф/версия Cursor это поддерживает).

Практика:
— ищи настройки в Cursor Settings, раздел про Models / API Keys / Providers.
— цель: чтобы Cursor мог использовать выбранную модель для Plan/Agent, а проект (Analyzer Machine) — использовать свой ключ только внутри Python (через .env).

Важно:
— ключ Cursor (для IDE) и ключ проекта (.env для Python) — это разные ключи и разные контуры доступа.
— ключи проекта никогда не коммитятся и не индексируются (.gitignore/.cursorindexingignore уже настроены).

8) Шаблоны: готовые промпты для Cursor
8.1. Plan (декомпозиция)
Составь план реализации <FEATURE> для Analyzer Machine.
Условия:
— расчёты только кодом
— минимизируем запросы, используем data_cache
— перечисли файлы, которые нужно создать/изменить
— добавь DoD с командами проверки
Результат оформи как docs/tasks/<ID>_<feature>.md

8.2. Agent (реализация по плану)
Реализуй задачу строго по docs/tasks/<ID>_<feature>.md.
Условия:
— не меняй API/CLI без необходимости
— не выводи секреты
— добавь/обнови кэш-файлы и нормализацию
— обнови docs/spec.md и docs/api_catalog.md, если добавляешь запросы/команды
После правок укажи, какие команды я должен запустить для проверки.

8.3. Debug
Вот команда, которую я запускал:
<команда>

Вот вывод/traceback:
<лог>

Найди причину, предложи минимальный фикс, и дай команды для повторной проверки.

9) Рекомендованный ближайший шаг для Analyzer Machine
Стабильный “первый вертикальный срез” после metrika-sources:
— команда analyze, которая сравнивает два периода по источникам (и далее расширяем).
Приоритет:
— сначала: сравнение периодов (дельты/вклады) в коде + вывод таблиц/JSON
— затем: добавление 1 LLM-вывода по агрегатам (опционально)
— затем: расширение на цели/конверсии

